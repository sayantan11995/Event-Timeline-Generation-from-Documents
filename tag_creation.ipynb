{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from lxml.html import fromstring\n",
    "from requests import get\n",
    "from urllib.parse import urlencode, urlparse, parse_qs\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "import spacy\n",
    "import pytextrank\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = pd.read_csv(\"CWAL.csv\", nrows=423)\n",
    "\n",
    "# event_data = pd.read_csv(\"covid19/covid_data_with_tag.csv\")\n",
    "\n",
    "# event_data = event_data[event_data['sentence'].notna()]\n",
    "\n",
    "# event_data = event_data.loc[(event_data.importance!=0) & (event_data.tag!='Other')]\n",
    "# event_data.shape\n",
    "\n",
    "# event_data.head()\n",
    "event_data.cluster.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "\n",
    "\n",
    "actions = []\n",
    "\n",
    "for sent in event_data.sentence.to_list():\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    hw = [x.lemma_ for x in [y \n",
    "                            for y\n",
    "                            in doc\n",
    "                            if not y.is_stop and y.pos_ != 'PUNCT'] if x.pos_ == 'VERB' and x.dep_ == 'ROOT']\n",
    "\n",
    "    hw = list(set(hw))\n",
    "\n",
    "    actions.append(hw)\n",
    "    \n",
    "for i in range(len(actions)):\n",
    "    actions[i] = str(actions[i])[1:-1]\n",
    "    \n",
    "event_data['action'] = pd.Series(actions)\n",
    "\n",
    "event_data['index'] = event_data.index\n",
    "event_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_file = file.loc[(file.tag=='event') | (file.tag=='fact') ]\n",
    "event_data = event_data[event_data['sentence'].notna()]\n",
    "event_data = event_data[event_data['cluster'].notna()]\n",
    "event_data.head(), event_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "!pip install google\n",
    "!pip install rake_nltk --user\n",
    "!pip install pytextrank\n",
    "from googlesearch import search\n",
    "import requests\n",
    "\n",
    "\n",
    "def google_scrape(url):\n",
    "    thepage = requests.get(str(url)).text\n",
    "    soup = BeautifulSoup(thepage, \"html.parser\")\n",
    "    try:\n",
    "        txt = soup.title.text\n",
    "    except:\n",
    "        txt = \"\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "#get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Adding custom stopwords\n",
    "en_stopwords.update({'LETTER', 'COLLECTED', 'WORKS', 'MAHATMA',  'GANDHI', '|'})\n",
    "\n",
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "tag_list = []\n",
    "\n",
    "\n",
    "for i in event_data.index:\n",
    "    \n",
    "    print(\"%s processing\"%i)\n",
    "    \n",
    "    query = str(event_data.sentence[i]).strip()\n",
    "#     time.sleep(5)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    my_results_list = []\n",
    "    for url in search(query + 'gandhi',        # The query you want to run\n",
    "                    tld = 'com',  # The top level domain\n",
    "                    lang = 'en',  # The language\n",
    "                    num = 5,     # Number of results per page\n",
    "                    start = 0,    # First result to retrieve\n",
    "                    stop = 5,\n",
    "                    safe='on',  # Last result to retrieve\n",
    "                    pause = 5.0,  # Lapse between HTTP requests\n",
    "                ):\n",
    "        try:\n",
    "            r = google_scrape(url)\n",
    "        except:\n",
    "            r = ''\n",
    "        my_results_list.append(r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    text = query + \" \" + \" \".join(my_results_list)\n",
    "    \n",
    "    file = str(event_data['book no.'][i]).strip()\n",
    "    with open(str(os.path.join('text_files_combined', file)), 'r', encoding=\"utf8\") as content:\n",
    "        raw_data = content.read()\n",
    "\n",
    "    raw_data = str(event_data.Text[i])\n",
    "\n",
    "    text =  raw_data + \" \".join(my_results_list) \n",
    "    print(text)\n",
    "\n",
    "    text = text.replace('LETTER TO', '')\n",
    "    text = text.replace('COLLECTED WORKS', ' ')\n",
    "    text = text.replace('MAHATMA GANDHI', '')\n",
    "\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(elapsed)\n",
    "\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "    #bigrams\n",
    "    bigram_freq = bigramFinder.ngram_fd.items()\n",
    "    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "    #filter bigrams\n",
    "    filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "    tags = filtered_bi['bigram'].tolist()[:15]\n",
    "    tags = [\" \".join(x) for x in tags]\n",
    "    print(query)\n",
    "    print(tags)\n",
    "\n",
    "    tag_list.append(tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing '[' from tag\n",
    "  \n",
    "for i in range(len(tag_list)):\n",
    "    tag_list[i] = str(tag_list[i])[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_data.sentence[event_data.index[0]]\n",
    "\n",
    "# tag_list1 = tag_list\n",
    "\n",
    "# len(tag_list)\n",
    "tag_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_data.keywords = event_data.keywords.fillna('null')\n",
    "# for i in event_data.index:\n",
    "#     if event_data.keywords[i] == 'null':\n",
    "#         event_data.keywords[i] = tag_list[i]\n",
    "# event_data = event_data.loc[(event_data.importance!=0) & (event_data.tag!='Other')]\n",
    "event_data['keywords_unfiltered'] = pd.Series(tag_list)\n",
    "\n",
    "# event_data['index'] = event_data.index\n",
    "event_data.head()\n",
    "event_data.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tr = pytextrank.TextRank(logger=None)\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "\n",
    "tags = []\n",
    "for query in event_file.sentence[50:52]:\n",
    "    # search_results = google.search(query, 2)\n",
    "    # time.sleep(5)\n",
    "    # text = query + \" \"\n",
    "    # for res in search_results:\n",
    "    #     text += res.name + \" \"\n",
    "    #     text += res.description + \" \"\n",
    "\n",
    "    my_results_list = []\n",
    "    for url in search(query + 'gandhi',        # The query you want to run\n",
    "                    tld = 'com',  # The top level domain\n",
    "                    lang = 'en',  # The language\n",
    "                    num = 5,     # Number of results per page\n",
    "                    start = 0,    # First result to retrieve\n",
    "                    stop = 5,\n",
    "                    safe='on',  # Last result to retrieve\n",
    "                    pause = 5.0,  # Lapse between HTTP requests\n",
    "                ):\n",
    "        try:\n",
    "            r = google_scrape(url)\n",
    "        except:\n",
    "            r = ''\n",
    "        my_results_list.append(r)\n",
    "        \n",
    "    text = query + \" \" + \" \".join(my_results_list)\n",
    "    \n",
    "    nlp.max_length = 10**7\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    doc = nlp(preprocessed_text)\n",
    "    tag_list = []\n",
    "    # for p in doc._.phrases[:10]:\n",
    "    #     tag_list.append(p.text)\n",
    "    # tags.append(tag_list)\n",
    "    rake = Rake(max_length=3)\n",
    "    rake.extract_keywords_from_text(preprocessed_text)\n",
    "    tags.append(rake.get_ranked_phrases_with_scores()[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_tag_list = []\n",
    "my_tag_list += tags\n",
    "my_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = [[]]* event_file.shape[0]\n",
    "for i in range(len(my_tag_list)):\n",
    "    tag_list[i] = my_tag_list[i]\n",
    "event_file['knowledge_tag'] = tag_list\n",
    "\n",
    "# event_file.shape\n",
    "event_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_file.to_csv('sentence_with_knowledge_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rake_nltk import Rake\n",
    "tr = pytextrank.TextRank(logger=None)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "nlp.max_length = 10**7\n",
    "preprocessed_text = preprocess_text(text)\n",
    "doc = nlp(preprocessed_text)\n",
    "\n",
    "for p in doc._.phrases[:10]:\n",
    "    print(\"{}\".format(p.text))\n",
    "    \n",
    "rake = Rake(max_length=2)\n",
    "rake.extract_keywords_from_text(preprocessed_text)\n",
    "print(rake.get_ranked_phrases_with_scores()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = event_file.loc[(file.tag=='event') | (file.tag=='fact') | (file.tag=='demand')]\n",
    "selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = str(event_file.sentence[2]).strip()\n",
    "\n",
    "# time.sleep(5)\n",
    "# search_results = google.search(q, 3)\n",
    "\n",
    "# text = query + \" \"\n",
    "# for res in search_results:\n",
    "#     print(1)\n",
    "#     text += res.name + \" \"\n",
    "#     text += res.description + \" \"\n",
    "    \n",
    "# text += event_file.Text[2]\n",
    "\n",
    "text = event_file.Text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#get english stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in en_stopwords or word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "tag_list = []\n",
    "\n",
    "for i in event_file.id: \n",
    "    if event_file.tag[i] == 'event' or event_file.tag[i] == 'fact' or event_file.tag[i] == 'demand':\n",
    "        text = preprocess_text(event_file.Text[i])\n",
    "        tokens = nltk.tokenize.word_tokenize(text + event_file.sentence[i])\n",
    "        bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "    #     trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "        # change this to read in your data\n",
    "        # finder = BigramCollocationFinder.from_words(file.Text[0])\n",
    "        # # only bigrams that appear 3+ times\n",
    "        # finder.apply_freq_filter(3) \n",
    "\n",
    "        # return the 5 n-grams with the highest PMI\n",
    "        # finder.nbest(bigram_measures.pmi, 5)  \n",
    "\n",
    "        #bigrams\n",
    "        bigram_freq = bigramFinder.ngram_fd.items()\n",
    "        bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "        #filter bigrams\n",
    "        filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "        tags = filtered_bi['bigram'].tolist()\n",
    "        tags = [\" \".join(x) for x in tags]\n",
    "\n",
    "        tag_list.append(tags[:10])\n",
    "    else:\n",
    "        tag_list.append(np.nan)\n",
    "\n",
    "# #trigrams\n",
    "# trigram_freq = trigramFinder.ngram_fd.items()\n",
    "# trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "# #function to filter for trigrams\n",
    "# def rightTypesTri(ngram):\n",
    "#     if '-pron-' in ngram or 't' in ngram:\n",
    "#         return False\n",
    "#     for word in ngram:\n",
    "#         if word in en_stopwords or word.isspace():\n",
    "#             return False\n",
    "#     first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "#     third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "#     tags = nltk.pos_tag(ngram)\n",
    "#     if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "# #filter trigrams\n",
    "# filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tag_list)):\n",
    "    tag_list[i] = str(tag_list[i])[1:-1]\n",
    "\n",
    "event_file['keywords'] = tag_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "# #filter for only those with more than 20 occurences\n",
    "# bigramFinder.apply_freq_filter(20)\n",
    "# trigramFinder.apply_freq_filter(20)\n",
    "bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigram_measures.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n",
    "# trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(bigram_measures.pmi)), columns=['trigram','PMI']).\\\n",
    "#                                     sort_values(by='PMI', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramPMITable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79343538e6373866518bae8914e0fa7d73ef4b03ba66fe6bca0ffef2de4804d5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
